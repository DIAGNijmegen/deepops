#
# Example slurm.conf file generated by DeepOps. 
#
# Slurm provides configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
# See the slurm.conf man page for more information.

# Define a name for the cluster
ClusterName=deepops

# Configure the controllers
SlurmctldHost=master-malamar
StateSaveLocation=/var/spool/slurm/ctld

# Basic configuration
SlurmUser=slurm
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
PluginDir=/usr/local/lib/slurm
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
PropagateResourceLimitsExcept=MEMLOCK

# Basic job behavior
ReturnToService=1
RebootProgram="/bin/systemctl reboot"
ResumeTimeout=900

# Use PMIX as our default MPI configuration
MpiDefault=pmix

# Prolog/epilog config
PrologFlags=Alloc,Serial,Contain
Prolog=/etc/slurm/prolog.sh
Epilog=/etc/slurm/epilog.sh
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=

# Health checking
HealthCheckProgram=/usr/sbin/nhc
HealthCheckInterval=300
HealthCheckNodeState=IDLE

# Mail program to use
MailProg=/usr/bin/s-nail

TaskPlugin=affinity,cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=

# TIMERS
SlurmctldTimeout=120
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=90
Waittime=0
UnkillableStepTimeout=180

# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
PriorityType=priority/multifactor
PriorityWeightQOS=10000
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0

# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=

# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
AccountingStorageTRES=gres/gpu
#DebugFlags=CPU_Bind,gres
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=master-malamar
#AccountingStorageLoc=
AccountingStorageEnforce=associations,limits,qos
AccountingStorageUser=slurm
AccountingStoragePass=/var/run/munge/munge.socket.2


# COMPUTE NODES
GresTypes=gpu
NodeName=dlc-articuno Gres=gpu:nvidia_geforce_rtx_2080_ti:8 CPUs=88 Sockets=2 CoresPerSocket=22 ThreadsPerCore=2 Procs=44 RealMemory=366292 State=UNKNOWN
#NodeName=dlc-jynx Gres=gpu:nvidia_geforce_gtx_titan_x:4 CPUs=12 Sockets=1 CoresPerSocket=6 ThreadsPerCore=2 Procs=6 RealMemory=60990 State=UNKNOWN
#NodeName=dlc-magmar Gres=gpu:4 CPUs=12 Sockets=1 CoresPerSocket=6 ThreadsPerCore=2 Procs=6 RealMemory=244759 State=UNKNOWN
NodeName=dlc-drowzee Gres=gpu:nvidia_geforce_gtx_1080:2 CPUs=12 Sockets=1 CoresPerSocket=6 ThreadsPerCore=2 Procs=6 RealMemory=244768 State=UNKNOWN
NodeName=dlc-groudon Gres=gpu:nvidia_a100-pcie-40gb:2 CPUs=112 Sockets=2 CoresPerSocket=28 ThreadsPerCore=2 Procs=56 RealMemory=367254 State=UNKNOWN
#NodeName=dlc-togepi Gres=gpu:4 CPUs=40 Sockets=2 CoresPerSocket=10 ThreadsPerCore=2 Procs=20 RealMemory=244919 State=UNKNOWN
NodeName=dlc-tyranitar Gres=gpu:nvidia_geforce_rtx_2080_ti:1 CPUs=16 Sockets=1 CoresPerSocket=8 ThreadsPerCore=2 Procs=8 RealMemory=61030 State=UNKNOWN
NodeName=dlc-tornadus Gres=gpu:nvidia_geforce_rtx_3080_ti:8 CPUs=96 Sockets=2 CoresPerSocket=24 ThreadsPerCore=2 Procs=48 RealMemory=489825 State=UNKNOWN
NodeName=dlc-electabuzz Gres=gpu:nvidia_geforce_gtx_titan_x:1 CPUs=20 Sockets=1 CoresPerSocket=10 ThreadsPerCore=2 Procs=10 RealMemory=122284 State=UNKNOWN
NodeName=dlc-scyther Gres=gpu:nvidia_geforce_gtx_titan_x:1 CPUs=12 Sockets=1 CoresPerSocket=6 ThreadsPerCore=2 Procs=6 RealMemory=30430 State=UNKNOWN
NodeName=dlc-venusaur Gres=gpu:nvidia_geforce_gtx_1080_ti:3 CPUs=40 Sockets=2 CoresPerSocket=10 ThreadsPerCore=2 Procs=20 RealMemory=248000 State=UNKNOWN
#NodeName=dlc-magneton Gres=gpu:4 CPUs=12 Sockets=1 CoresPerSocket=6 ThreadsPerCore=2 Procs=6 RealMemory=244759 State=UNKNOWN
NodeName=dlc-nidoking Gres=gpu:nvidia_geforce_rtx_2080_ti:8 CPUs=80 Sockets=2 CoresPerSocket=20 ThreadsPerCore=2 Procs=40 RealMemory=244684 State=UNKNOWN
#NodeName=dlc-starmie Gres=gpu:2 CPUs=32 Sockets=1 CoresPerSocket=16 ThreadsPerCore=2 Procs=16 RealMemory=60954 State=UNKNOWN
#NodeName=dlc-charizard Gres=gpu:4 CPUs=40 Sockets=2 CoresPerSocket=10 ThreadsPerCore=2 Procs=20 RealMemory=244919 State=UNKNOWN
NodeName=dlc-lugia Gres=gpu:nvidia_geforce_rtx_2080_ti:8 CPUs=88 Sockets=2 CoresPerSocket=22 ThreadsPerCore=2 Procs=44 RealMemory=385572 State=UNKNOWN
NodeName=dlc-moltres Gres=gpu:nvidia_geforce_rtx_2080_ti:8 CPUs=88 Sockets=2 CoresPerSocket=22 ThreadsPerCore=2 Procs=44 RealMemory=256836 State=UNKNOWN
NodeName=dlc-zapdos Gres=gpu:nvidia_geforce_rtx_2080_ti:8 CPUs=88 Sockets=2 CoresPerSocket=22 ThreadsPerCore=2 Procs=44 RealMemory=248000 State=UNKNOWN
NodeName=dlc-meowth Gres=gpu:nvidia_l40s:4 CPUs=96 Sockets=2 CoresPerSocket=24 ThreadsPerCore=2 Procs=48 RealMemory=248000 State=UNKNOWN

# hardcoding the partitions and default memory per node
# TODO: automatically define the partitions by resource
# TODO: set DefMemPerCPU = TotalMemory / LogicalCPUs
PartitionName=batch Nodes=ALL Default=YES DefMemPerCPU=0 State=UP OverSubscribe=NO MaxTime=7-0:00 DefaultTime=0-4:00

# QoS settings
PreemptMode=REQUEUE
PreemptType=preempt/qos
JobRequeue=1
PreemptExemptTime=0
PreemptParameters=strict_order,youngest_first
